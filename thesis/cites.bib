@article{Smith2017,
abstract = {A tight lower bound for required I/O when computing a matrix-matrix multiplication on a processor with two layers of memory is established. Prior work obtained weaker lower bounds by reasoning about the number of $\backslash$textit{\{}phases{\}} needed to perform {\$}C:=AB{\$}, where each phase is a series of operations involving {\$}S{\$} reads and writes to and from fast memory, and {\$}S{\$} is the size of fast memory. A lower bound on the number of phases was then determined by obtaining an upper bound on the number of scalar multiplications performed per phase. This paper follows the same high level approach, but improves the lower bound by considering {\$}C:=AB+C{\$} instead of {\$}C:=AB{\$}, and obtains the maximum number of scalar fused multiply-adds (FMAs) per phase instead of scalar additions. Key to obtaining the new result is the decoupling of the per-phase I/O from the size of fast memory. The new lower bound is {\$}2mnk/\backslashsqrt{\{}S{\}}-2S{\$}. The constant for the leading term is an improvement of a factor {\$}4\backslashsqrt{\{}2{\}}{\$}. A theoretical algorithm that attains the lower bound is given, and how the state-of-the-art Goto's algorithm also in some sense meets the lower bound is discussed.},
archivePrefix = {arXiv},
arxivId = {1702.02017},
author = {Smith, Tyler Michael and van de Geijn, Robert A.},
eprint = {1702.02017},
file = {:home/krzys/Documents/fall-2017/thesis-papers/Pushing the Bounds for Matrix-Matrix Multiplication - Smith, van de Geijn.pdf:pdf},
mendeley-groups = {FLAME and BLIS},
OPTpages = {1--11},
title = {{Pushing the Bounds for Matrix-Matrix Multiplication}},
url = {http://arxiv.org/abs/1702.02017},
year = {2017}
}

@article{Gunnels2006,
abstract = {During the last half-decade, a number of research efforts have centered around developing software for generating automatically tuned matrix multiplication kernels. These include the PHiPAC project and the ATLAS project. The software end-products of both projects employ brute force to search a parameter space for blockings that accommodate multiple levels of memory hierarchy. We take a different approach: using a simple model of hierarchical memories we employ mathematics to determine a locally-optimal strategy for blocking matrices. The theoretical results show that, de-pending on the shape of the matrices involved, different strategies are locally-optimal. Rather than determining a blocking strategy at library generation time, the theoret-ical results show that, ideally, one should pursue a heuristic that allows the blocking strategy to be determined dynamically at run-time as a function of the shapes of the operands. When the resulting family of algorithms is combined with a highly optimized inner-kernel for a small matrix multiplication, the approach yields performance that is superior to that of methods that automatically tune such kernels. Preliminary results, for the Intel Pentium (R) III processor, support the theoretical insights.},
author = {Gunnels, John A. and Gustavson, Fred G. and Henry, Greg M. and {Van De Geijn}, Robert A.},
doi = {10.1007/11558958_30},
file = {:home/krzys/Documents/fall-2017/thesis-papers/A family of high-performance matrix multiplication algorithms - Gunnels et al.pdf:pdf},
isbn = {3540290672},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {FLAME and BLIS},
OPTpages = {256--265},
title = {{A family of high-performance matrix multiplication algorithms}},
volume = {3732 LNCS},
year = {2006}
}

@article{Low2016,
author = {Low, Tze Meng and Igual, Francisco D. and Smith, Tyler M. and Quintana-Orti, Enrique S.},
doi = {10.1145/2925987},
file = {:home/krzys/Documents/fall-2017/thesis-papers/Analytical Modeling Is Enough for High-Performance BLIS - Low et al.pdf:pdf},
issn = {0098-3500},
journal = {ACM Transactions on Mathematical Software},
mendeley-groups = {FLAME and BLIS},
number = {2},
OPTpages = {1--18},
title = {{Analytical Modeling Is Enough for High-Performance BLIS}},
url = {http://dl.acm.org/citation.cfm?doid=2988256.2925987},
volume = {43},
year = {2016}
}

@phdthesis{SmithDiss2017,
author = {Smith, Tyler Michael},
file = {:home/krzys/Documents/fall-2017/thesis-papers/SMITH-DISSERTATION-2017.pdf:pdf},
mendeley-groups = {FLAME and BLIS},
school = {The University of Texas at Austin},
title = {{Theory and Practice of Classical Matrix-Matrix Multiplication for Hierarchical Memory Architectures}},
type = {PhD},
url = {http://hdl.handle.net/2152/63352},
year = {2018}
}

@article{Goto2008,
abstract = {We present the basic principles that underlie the high-performance implementation of the matrix-matrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve near-peak performance.},
author = {Goto, Kazushige and van de Geijn, Robert A.},
doi = {10.1145/1356052.1356053},
file = {:home/krzys/Documents/fall-2017/thesis-papers/GotoTOMS{\_}revision.pdf:pdf},
isbn = {9780769552071},
issn = {00983500},
journal = {ACM Transactions on Mathematical Software},
mendeley-groups = {FLAME and BLIS},
number = {3},
OPTpages = {1--25},
title = {{Anatomy of high-performance matrix multiplication}},
url = {http://portal.acm.org/citation.cfm?doid=1356052.1356053},
volume = {34},
year = {2008}
}

@inproceedings{Barthels2018,
author = {Barthels, Henrik and Copik, Marcin and Bientinesi, Paolo},
booktitle = {Proceedings of the 2018 International Symposium on Code Generation and Optimization},
doi = {10.1145/3168804},
file = {:home/krzys/Documents/fall-2017/thesis-papers/The Generalized Matrix Chain Algorithm - Barthels, Copik, Bientinesi.pdf:pdf},
isbn = {978-1-4503-5617-6},
keywords = {compiler,linear algebra,matrix chain problem},
mendeley-groups = {FLAME and BLIS},
OPTpages = {138--148},
publisher = {ACM},
title = {{The Generalized Matrix Chain Algorithm}},
url = {https://dl.acm.org/citation.cfm?id=3168804},
year = {2018}
}

@article{Hu1984,
author = {Hu, TC and Shing, MT},
doi = {10.1137/0213017},
file = {:home/krzys/Documents/fall-2017/thesis-papers/0213017.pdf:pdf},
journal = {SIAM Journal on Computing},
mendeley-groups = {FLAME and BLIS},
number = {2},
OPTpages = {228----251},
title = {{Computation of matrix chain products. Part II}},
url = {http://epubs.siam.org/doi/pdf/10.1137/0213017},
volume = {13},
publisher = {SIAM},
year = {1984}
}

@Article{Choi1995,
author="Choi, Jaeyoung
and Dongarra, Jack J.
and Walker, David W.",
title="The design of a parallel dense linear algebra software library: Reduction to Hessenberg, tridiagonal, and bidiagonal form",
journal="Numerical Algorithms",
year="1995",
OPTmonth="Sep",
OPTday="01",
volume="10",
number="2",
OPTpages="379--399",
abstract="This paper discusses issues in the design of ScaLAPACK, a software library for performing dense linear algebra computations on distributed memory concurrent computers. These issues are illustrated using the ScaLAPACK routines for reducing matrices to Hessenberg, tridiagonal, and bidiagonal forms. These routines are important in the solution of eigenproblems. The paper focuses on how building blocks are used to create higher-level library routines. Results are presented that demonstrate the scalability of the reduction routines. The most commonly-used building blocks used in ScaLAPACK are the sequencing BLAS, the parallel BLAS (PBLAS) and the Basic Linear Algebra Communication Subprograms (BLACS). Each of the matrix reduction algorithms consists of a series of steps in each of which one block column (orpanel), and/or block row, of the matrix is reduced, followed by an update of the portion of the matrix that has not been factorized so far. This latter phase is performed using Level 3 PBLAS operations and contains the bulk of the computation. However, the panel reduction phase involves a significant amount of communication, and is important in determining the scalability of the algorithm. The simplest way to parallelize the panel reduction phase is to replace the BLAS routines appearing in the LAPACK routine (mostly matrix-vector and matrix-matrix multiplications) with the corresponding PBLAS routines. However, in some cases it is possible to reduce communication startup costs by performing the communication necessary for consecutive BLAS operations in a single communication using a BLACS call. Thus, there is a tradeoff between efficiency and software engineering considerations, such as ease of programming and simplicity of code.",
issn="1572-9265",
doi="10.1007/BF02140776",
url="https://doi.org/10.1007/BF02140776"
}

@article{Rao2017,
author = {Rao, Vishwas and Sandu, Adrian and Ng, Michael and Nino-Ruiz, Elias D},
doi = {10.1137/15M1045910},
file = {:home/krzys/Documents/fall-2017/thesis-papers/15m1045910.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
keywords = {4D-var,ADMM,Huber norm5463215674,data assimilation},
mendeley-groups = {FLAME and BLIS},
number = {3},
OPTpages = {548--570},
title = {{Robust Data Assimilation Using $L_1$ and Huber Norms}},
volume = {39},
year = {2017},
url = {https://epubs.siam.org/doi/pdf/10.1137/15M1045910}
}

@inproceedings{Barthels2017,
address = {Kaiserslautern},
author = {Barthels, Henrik and Bientinesi, Paolo},
booktitle = {The 8th International Workshop on Parallel Symbolic Computation},
file = {:home/krzys/Documents/fall-2017/thesis-papers/Linnea Compiling Linear Algebra Expressions to High-Performance Code - Barthels.pdf:pdf},
keywords = {2017,acm reference format,bientinesi,code generation,compiler,compiling linear al-,henrik barthels and paolo,linear algebra,linnea},
mendeley-groups = {FLAME and BLIS},
publisher = {ACM},
title = {{Linnea: Compiling Linear Algebra Expressions to High-Performance Code}},
isbn = {978-1-4503-5288-8},
doi = {10.1145/3115936.3115937},
year = {2017},
url = {http://hpac.rwth-aachen.de/~barthels/publications/PASCO_2017.pdf}
}

@inproceedings{Whaley1998,
author = {Whaley, Clint R and Dongarra, Jack J},
booktitle = {Proceedings of the 1998 ACM/IEEE conference on Supercomputing},
file = {:home/krzys/Documents/fall-2017/thesis-papers/10.1.1.143.3947.pdf:pdf},
OPTpages = {1----27},
publisher = {ACM},
isbn = {0-89791-984-X},
url = {https://dl.acm.org/citation.cfm?id=509096},
title = {{Automatically Tuned Linear Algebra Software}},
year = {1998}
}

@article{VanZee2016,
author = {van Zee, Field G and Smith, Tyler M and Marker, Bryan and Low, Tze Meng and van de Geijn, Robert A. and Igual, Francisco D and Smelyanskiy, Makhail and Zhang, Xianyi and Kistler, Michael and Austel, Vernon and Gunnels, John A. and Killough, Lee},
file = {:home/krzys/Documents/fall-2017/thesis-papers/a12-zee.pdf:pdf},
journal = {ACM Transactions on Mathematical Software},
keywords = {BLAS2,Linear algebra,high performance,libraries,matrix,multiplication},
mendeley-groups = {FLAME and BLIS},
number = {2},
title = {{The BLIS Framework: Experiments in Portability}},
volume = {42},
year = {2016},
publisher = {ACM},
doi = {10.1145/2755561},
url = {https://dl.acm.org/citation.cfm?doid=2936306.2755561}
}

@TechReport{Poulson2011,
  author = {Jack Poulson and Robert van de Geijn and Jeffrey Bennighof},
  title = {Parallel Algorithms for Reducing the Generalized Hermitian-Definite Eigenvalue Problem.  {FLAME} {W}orking {N}ote \#56},
  institution = {The University of Texas at Austin, Department of Computer Sciences},
  type = {Technical Report},
  number = {TR-11-05},
  OPTmonth = FEB,
  year = {2011},
  url = {https://www.cs.utexas.edu/users/flame/pubs/FLAWN56.pdf}
}

@unpublished{Parikh2017,
address = {Austin},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.04286v1},
author = {Parikh, Devangi N and Myers, Margaret E and Geijn, Robert A Van De},
eprint = {arXiv:1710.04286v1},
file = {:home/krzys/Documents/fall-2017/thesis-papers/1710.04286.pdf:pdf},
institution = {University of Texas at Austin},
mendeley-groups = {FLAME and BLIS},
OPTpages = {1--12},
series = {FLAME Working Notes},
title = {{Deriving Correct High-Performance Algorithms}},
url = {https://arxiv.org/abs/1710.04286},
year = {2017}
}

@TechReport{Henry92,
  author = {Greg Henry},
  title = {{BLAS} based on block data structures},
  institution =  {Cornell University},
  year = {1992},
  type = {Theory Center Technical Report},
  number = {CTC92TR89},
  url = {http://hdl.handle.net/1813/5471},
  OPTmonth = {2},
}

@BOOK{lapack_ug,
      AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
                Blackford, S. and Demmel, J. and Dongarra, J. and
                Du Croz, J. and Greenbaum, A. and Hammarling, S. and
                McKenney, A. and Sorensen, D.},
      TITLE = {{LAPACK} Users' Guide},
      EDITION = {Third},
      PUBLISHER = {Society for Industrial and Applied Mathematics},
      YEAR = {1999},
      ADDRESS = {Philadelphia, PA},
      ISBN = {0-89871-447-8 (paperback)},
      DOI = {10.1137/1.9780898719604},
      URL = {https://epubs.siam.org/doi/book/10.1137/1.9780898719604}
}

@article{blas_standard,
author = {Jack J. Dongarra and Du Croz, Jeremy and Sven Hammarling and
Iain Duff},
title = {A Set of Level 3 Basic Linear Algebra Subprograms},
journal = {Transactions on Mathematical Systems},
volume = 16,
number = 1,
OPTpages = {1-17},
year = 1990,
doi = {10.1145/77626.79170},
url = {https://dl.acm.org/citation.cfm?id=79170}
}


article{blas_standard,
  title={Basic linear algebra subprograms technical {(BLAST)} forum standard},
  author={Dongarra, Jack},
  journal={The International Journal of High Performance Computing Applications},
  volume={16},
  number={2},
  pages={115--115},
  year={2002},
  publisher={SAGE Publications Sage UK: London, England}
}

techreport{Choi1995,
author = {Choi, Jaeyoung and Dongarra, Jack J and Walker, David W},
file = {:home/krzys/Documents/fall-2017/thesis-papers/The{\_}Design{\_}of{\_}a{\_}Parallel{\_}Dense{\_}Linear{\_}Algebra{\_}Soft.pdf:pdf},
institution = {Oak Ridge National Laboratory},
mendeley-groups = {FLAME and BLIS},
title = {{The Design of a Parallel Dense Linear Algebra Software Library: Reduction To Hessenberg, Tridiagonal, and Bidiagonal Form}},
year = {1995}
}
