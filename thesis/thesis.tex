\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{mathtools,amsthm}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{fontspec}
\usepackage{microtype}

\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\newcommand*{\TO}{\textbf{to}}
% since gemm3 can't be a matro name
\newcommand*{\pluseq}{\mathrel{{+}{=}}}
\newcommand*{\gemmt}{{\textsc{gemm3()}}}
\newcommand*{\gemm}{{\textsc{gemm()}}}

\usepackage{hyperref}
\usepackage{biblatex}

\addbibresource{cites.bib}

\usepackage{setspace}
%\singlespacing{}
\doublespacing{}

\usepackage{graphicx}

\usepackage{tikz}
\usetikzlibrary{arrows.meta,calc,fit,positioning,chains}
\input{libgemmpicture}

\title{\gemmt{}: Constant-workspace high-performance multiplication of three matrices for matrix chaining}
\author{Krzysztof A. Drewniak}

\begin{document}
\maketitle{}
\section{Introduction}
The matrix chain problem asks how to efficiently parenthesize a matrix product $A_1A_2\cdots A_n$.
$O(n \log n)$ algorithms for solving this problem are known\cite{Hu1984}, as are simpler $O(n^3)$ dynamic-programming solutions\cite{Barthels2018}
These algorithm all assume that the only operation available is the multiplication of two matrices.

In practice, a more general version of this problem appears, where matrices can be transposed and/or inverted, or have properties such as being upper triangular\cite{Barthels2018}.
For example, an algorithm to compute the ensemble Kalman filter\cite{Rao2017} (which computes parameters for physical systems from noisy data) requires the computation of the expression $X_i^b S_i (Y_i^b)^T R_i^{-1}$, which features one transposition and one inverse.
Other examples of the generalized matrix chain problem include the expression $L^{-1}AL^{-H}$ (where $L$ is lower triangular and $A$ is symmetric), which arises in blocked algorithms for the two-sided triangular linear solve\cite{Parikh2017} and $\tau_u\tau_v vv^TAuu^T$ (where the $\tau_i$ are scalars and $v$ and $u$ are vectors), which appears in an algorithm for reducing a matrix to tridiagonal form\cite{Choi1995}.

The BLAS and LAPACK libraries provide many high-performance functions that can assist in the computation of such matrix chains, such as specialized functions for the multiplication of triangular matrices (\textsc{trmm}()).
However, manually determining how to use these functions and coding against them requires expert knowledge.
Existing high-level systems, such at Matlab and Julia, which are commonly used by non-experts, generally compute generalized matrix chains in a naive way, such as proceeding from left to right, which is often rather ineffecient.
Systems such as Linnea\cite{Barthels2017} have emerged recently that attack the generalized matrix chain problem and automatically generate a high-performance program for a given expression.

Returning to the example matrix chains above, we can observe that they often contain the multiplication of three matrices as a subproblem, often after some preprocessing such as an inversion or an outer product.
The general form of such a problem is $D \coloneqq \alpha ABC + \beta D$ (with some of the operanrds optionally transposed), which we will summarize as $D \pluseq ABC$ and denote \gemmt{}, by analogy with the notation for general matrix-matrix multiply (\gemm{}) in the BLAS library.
The only approach available to this subproblem with current methods is to paranthesize the expression $ABC$ and perform a pair of matrix multiplications, storing a result in a temporary matrix $T$.
This method has two drawbacks.
The first is that $T$ is often rather large and requires significant amounts of storage space.
In addition, if $T$ is sufficiently large, creating and using it incurs a non-negligible performance cost due to the large number of slow main-memory operations required.

To combat these issues, we have developed an algorithm for \gemmt{} which does not require an entire intermediate product to be stored at one time.
Our algorithm takes advantage of the blocking performed by modern \gemm{} implementations to only store a cache-sized block of $BC$ at any given time.
By doing so, we perform the multiplication in $O(1)$ additional space and maintain performance comparable with a pair of \gemm{} calls.
Our algorithm can therefore be used as a primitive in generalized matrix chain solvers (or hand-written matrix code) to reduce memory usage, decrease the complexity of the solution, and potentially provide a slight performance increase.

\section{Background}
\subsection{High-Performance \gemm{}}
Before discussing our approach to \gemmt{}, it is important to review the implementation of high-performance \gemm{}.
High-performance \gemm{} algorithms operate by repeatedly reducing the multiplication to a series of multiplications of blocks of the inputs.
These blocks are sized to allow an operand to one of these subproblems to utilize a level of the CPU's cache effectively and prevent it from being evicted during further blocking.
The multiple reductions are required to effectively utilize all levels of the cache.
These steps are necessary because of the memory hierarchy of modern CPUs, where there are multiple (typically three) levels of cache, which increase in size but decrease in speed, between registers and main memory.

There are two specialized primitives that appear in high-performance \gemm{}: the \emph{microkernel} and \emph{macrokernel}.
The microkernel is a highly-tuned, hand-written function (almost always implemented in assembly) that multiplies an $m_R \times k$ panel of $A$ by an $k \times n_R$ panel of $B$ to update an $m_R \times n_R$ region of $C$, which is computed in registers and written to memory.
$m_R$ and $n_R$ are constants based on the microarchitecture of the CPU, which can be derived analytically\cite{Low2016} or by autotuning\cite{Whaley1998}.

In order for the microkernel to operate efficiently, the data within the panels must be arranged so that it will read memory in a linear fashion, which is the pattern of reads best suited to the CPU's memory prefetcher.
This is achieved by storing each panel of $A$ in column-major form with columns of height $m_R$ and keeping each panel of $B$ row-major with row width $n_R$.

More importantly, to allow the microkernel to stream the data it operates on efficiently, the panels it examines must be stored in cache beforehand.
Since caches are fixed-size, we can only store a fixed number of such panels at any given time.
Specifically, we can only store an $m_C \times k_C$ block of $A$ and a $k_C \times n_C$ block of $B$.
The process of rearranging these blocks into the format needed by the microkernel is known as \emph{packing}.
The constants $m_C$, $k_C$, and $n_C$ are chosen to ensure all levels of cache are used efficiently.

We will use $\tilde{A}$ and $\tilde{B}$ to denote the packed blocks of $A$ and $B$, respectively, and $C'$ to denote the corresponding block of $C$.
The loops that perform $C' \pluseq \tilde{A}\tilde{B}$ form the macrokernel, which is depicted as Algorithm \ref{alg:macrokernel}.
(In all the algorithms presented in this work, we will use Python-style notation for indexing, that is, matrices are 0-indexed and   $M[a:b, c:d]$ selects rows $[a, b)$ and columns $[c, d)$, with omitted operands spanning to the beginning/end of the dimension.)

\begin{algorithm}
  \caption{The macrokernel of a high-performance \gemm{} implementation}
  \label{alg:macrokernel}
  \begin{tikzpicture}
    \input{macrokernel-picture}
  \end{tikzpicture}
  \begin{algorithmic}
    \Procedure{macrokernel}{$\tilde{A}, \tilde{B}, C'$}
    \For{$j \gets 0, n_R, \ldots$ \TO{} $n_C$}
    \For{$i \gets 0, m_R, \ldots$ \TO{} $m_C$}
    \State{using the microkernel}
    \State{$C'[i:i + m_R, j:j + n_R] \pluseq \tilde{A}[i:i+m_R,:] \cdot \tilde{B}[:,j:j+n_R]$}
    \EndFor{}
    \EndFor{}
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}

Many high-performance \gemm{} implementations in use today are based on Goto's algorithm\cite{Goto2008}.
One such implementation, which we have based our work on, is BLIS\cite{VanZee2016}.
The BLIS algorithm, which is an elaboration on Goto's algorithm, is presented as Algorithm \ref{alg:blis}..
It brings elements of $B$ into the $L3$ (and later $L1$) cache, while storing elements of $A$ in $L2$.
\begin{algorithm}
  \caption{The BLIS algorithm}
  \label{alg:blis}
  \begin{tikzpicture}
    \input{blis-picture}
  \end{tikzpicture}
  \begin{algorithmic}
    \Procedure{BLIS\_gemm}{$A, B, C$}
    \For{$j \gets 0, n_C, \ldots$ \TO{} $n$}
    \For{$p \gets 0, k_C, \ldots$ \TO{} $k$}
    \State{pack $B[p:p+k_C,j:j+n_C] \to\tilde{B}$}
    \For{$i \gets 0, m_C, \ldots$ \TO{} $m$}
    \State{pack $A[m:m+m_C,p:p+k_C] \to \tilde{A}$}
    \State{$\textsc{macrokernel}(\tilde{A}, \tilde{B}, C[i:i+m_C,j:j+n_C])$}
    \EndFor{}
    \EndFor{}
    \EndFor{}
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}

\subsection{Constant selection for the BLIS algorithm}\label{subsec:constants}
The constants $m_R$, $n_R$, $k_C$, $m_C$, and $n_C$ that appear in the BLIS algorithm are computed using an analytical model from \cite{Low2016}.
We will summarize the process of deriving some of these constants here.
The values of $m_R$ and $n_R$ determine the structure of the microkernel, and are derived from the width of vector registers on a given CPU along with the latency and throughput of fused-multiply-add (FMA) instructions.
Since we are reusing existing optimized microkernels, we will not detail the process of selecting these constants except to note that $m_R$ and $n_R$ can be swapped during the cache-constant selection process without sacrificing performance.

The process of selecting the remaining constants assumes that, for each cache level $i$, the $Li$ cache has is a $W_{Li}$-way set-associative cache with $N_{Li}$ sets and $C_{Li}$ bytes per cache line.
All the caches are also assumed to have a least-recently-used replacement policy.
$S_{elem}$ represents the size of a single element of the matrix for generality.

The first constant we can derive is $k_C$, as it appears deepest in the loop structure of the algorithm.
We know that we will load a different $n_R \times k_C$ panel of $\tilde{B}$ from $L1$ during each call to the microkernel.
We also know that the microkernel's reads from an $m_R \times k_C$ panel of $\tilde{A}$ will cause it to be resident in the $L1$ cache.
Finally, some values from $C'$ will also enter the cache, and must not evict the panels.

To prevent the panels of $\tilde{A}$ and $\tilde{B}$ from evicting each other from the $L1$ cache, we want them to reside in different ways within each cache set.
To achieve this, the panels of $\tilde{A}$ and $\tilde{B}$ must both have sizes that are integer multiples of $N_{L1}C_{L1}$.
This restriction, along with the size of the data in each panel, tells us that
\begin{equation*}
  m_Rk_CS_{elem} = C_AN_{L1}C_{L1}
\end{equation*}
for some integer $C_A$, and that the same relationship holds for $n_R$ and $B$.

Given the three sources of data in the $L1$ cache, it must be the case that $C_A + C_B + 1 \leq W_{L1}$ (the $1$ is for elements of $C'$), and that we want to maximize $C_B$ given this constraint.
Therefore, we have
\begin{equation*}
  C_B = \ceil*{\frac{n_Rk_CS_{elem}}{N_{L1}C_{L1}}} = \ceil*{\frac{n_R}{m_R}C_A}
\end{equation*}.
Manipulating the inequality further shows us that
\begin{equation*}
  C_A \leq \floor*{\frac{W_{L1} - 1}{1 + \frac{n_R}{m_R}}}
\end{equation*}

Choosing the largest possible value of $C_A$ that leaves $k_C$ an integer will maximize $k_C$, improving performance by increasing the amount of time spent in the microkernel.
It is as this point where $m_R$ and $n_R$ may need to be swapped.

Now that we have $k_C$, we can compute $m_C$ and $n_C$.
For $m_C$, we know that we need to reserve one way in the $L2$ cache for elements of $C'$, and $\ceil*{(n_Rk_CS_{elem})/(N_{L2}C_{L2})} = C_{B2}$ ways for the panel of $B$ in the $L1$ cache.
This allows us to bound $m_C$ by
\begin{equation*}
  m_C \leq \floor*{\frac{(W_{L2} - C_{B2} - 1)N_{L2}C_{L2}}{k_CS_{elem}}}
\end{equation*}
and then select $m_C$ as large as possible, keeping in mind that it must be divisible by $m_R$ for high performance.

Since the $L3$ cache is, in practice, very large and somewhat slow, $n_C$ is computed by finding the largest value such that $n_Ck_C$ is less that the size of the $L3$ cache, excluding an $L1$ cache's worth of space to allow elements of $C'$ or $\tilde{A}$ to pass through.

\subsection{Data reuse in  matrix multiplication}
Examining the loops in the BLIS algorithm, as was done in\cite{Low2016}, shows that each loop causes some data from the computation to be reused, that is, read or written it its entirety during each iteration of the loop.
For example, the micro-kernel reuses the small block of $C'$ that is stored in registers while looping over the panels.

Even though reuse occurs at each level of the algorithm, we will focus on the reuse of the packed buffers, as this is a key consideration for maintaining the performance of the algorithm\cite{Henry92}.
Packing into $\tilde{B}$ and $\tilde{A}$ requires time that is not usable for computation.
Therefore, reusing the packed buffers many times will lower the relative proportion of time spent on packing, increasing performance.

To improve reuse of $\tilde{B}$, we need the 1st loop around the macrokernel, which packs into $\tilde{A}$, to run many times, since each of those iterations covers computations that read the entirety of $\tilde{B}$.
That is, we need $m$ to be large, as small values of $m$ will hurt performance by reducing the time between successive $\tilde{B}$ packing operations.
Similarly, large values of $n$ allow the 2nd loop around the microkernel to execute many times, thus improving reuse of $\tilde{A}$, which is consumed by the 1st loop around the microkernel.

The only loops that iterate over $k$ are those that pack $\tilde{B}$ (the second loop around the macrokernel) and the microkernel.
The second loop around the macrokernel does not result in the reuse of any data that has been loaded into cache because it is the first loop that performs such loads.
The microkernel only reuses data in registers, and so does not reuse cached data.
However, if $k$ is much less than $k_C$, the microkernel will only iterate a few times, increasing the proportion of time spent on non-computational overhaed.
Therefore, the size of $k$ has little impact on the reuse of packed blocks, except when $k \ll k_C$.

This reasoning shows that the BLIS algorithm achieves its best performance when $m$ and $n$ are large and $k$ is not extremely small.

\section{Algorithm}
For our discussion of \gemmt{}, we well be considering the operation $D \pluseq ABC$, where $A$ is $m \times k$, $B$ is $k \times l$ and $C$ is $l \times n$.

To derive our algorithm for \gemmt{}, we began by considering the computation of $\textsc{gemm}(A, BC)$ with the BLIS algorithm and attempting to find ways to not need all of $BC$ at once.
We observed that elements of $BC$ are not needed until that matrix has been partitioned both horizontaly and vertically, creating a block of $BC$ thas is packed into $L3$ cache.
This led us to consider how we might compute a $k_C \times l$ by $l \times n_C$ block of $BC$ and pack it into $\tilde{BC}$ efficiently.

Looking again at the BLIS algorithm, but this time considering the computation of such a block, we noticed that the outer loop (over $n$) could be removed for this ``inner'' computation, since the output could never be wider than $n_C$.
Simply nesting two copies of the BLIS algorithm, however, would cause issues, even with one loop removed.
One such issue was that there was no guarantee that the inner algorithm (which computed blocks of $BC$) would place its results into cache.
Another difficulty came from the fact that we would still need to pack blocks of $BC$ into $\tilde{BC}$, incurring a cost in memory operations we hoped to avoid.

Fortunately, these problems were resolvable.
The microkernel could be made to write in the packed format, eliminating a large number of memory operations, by informing it that the output matrix was row-major with rows of width $n_R$.
To solve the cache-residency issue, we observed that, if we used an $n_C$ value that was half of the one from BLIS, both a packed block of $C$ and the output block of $BC$ could reside in the $L3$ cache simultaneously.
This change does not break the assumptions of the BLIS algorithm, as $n_C/2$ is still much larger than $k_C$ or $m_C$ in practice.

One possible issue with this approach is that the inner computation is a panel-matrix multiply, that is, the $m$ dimension is small, and $\tilde{C}$ would see little reuse.
This did have a performance impact, though it turned out to be small in practice.

There was one other adjustment to the BLIS algorithm's constants than needed to be made to enable the fusion to proceed efficiently.
In some cases, the BLIS microkernel's loops have a final iteration that considers fewer than $m_R$ rows (or $n_R$ columns) because not all matrices have sizes divisible by $m_R$ and $n_R$.
Computation of this fridge iteration can have a performance impact, as special processing is required to account for the unusual sizes.
In the fused BLIS algorithm, the value of $k_C$ (which is used for partitioning in the outer algorithm) becomes the $m$ dimension of the problem computed by the inner algorithm.
If $k_C$ is not divisible by $m_R$, we must (at the cost of slightly less optimal cache usage) reduce it slightly so that it is a multiple of $m_R$, thus ensuring we do not incur performance penalties every time we compute $\tilde{BC}$.
However, $l_C$, which is the inner algorithm's version of $k_C$, can be kept at the BLIS value, as these considerations are not relevant there.

The algorithm that resulted from this derivation is Algorithm \ref{alg:gemm3}, which is illustrated in Figure \ref{fig:gemm3}.

\begin{figure}
  \centering
  \includegraphics[height=8.75in]{gemm3-picture}
  \caption{Illustration of algorithm for \gemmt{}}
  \label{fig:gemm3}
\end{figure}
\begin{algorithm}
  \caption{Algorithm for \gemmt{}}
  \label{alg:gemm3}
  \begin{algorithmic}
    \Procedure{gemm3}{$A, B, C, D$}
    \For{$j \gets 0, n_C, \ldots$ \TO{} $n$}
    \For{$p \gets 0, k_C, \ldots$ \TO{} $k$}
    \For{$q \gets 0, l_C, \ldots$ \TO{} $l$}
    \State{pack $C[q:q+l_C,j:j+n_C] \to \tilde{C}$}
    \For{$i \gets 0, m_C, \ldots$ \TO{} $k_C$}
    \State{pack $B[i:i+m_C,q:q+l_C] \to \tilde{B}$}
    \State{$\textsc{macrokernel}(\tilde{B}, \tilde{C}, \tilde{BC})$}
    \Comment{writes in packed form}
    \EndFor{}
    \EndFor{}
    \For{$i \gets 0, m_C, \ldots$ \TO{} $m$}
    \State{pack $A[m:m+m_C,p:p+k_C] \to \tilde{A}$}
    \State{$\textsc{macrokernel}(\tilde{A}, \tilde{BC}, D[i:i+m_C,j:j+n_C])$}
    \EndFor{}
    \EndFor{}
    \EndFor{}
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}

It should be noted that this algorithm computes $D \pluseq A(BC)$.
In some cases, it is much more efficient (on account of the dimensions of the inputs) to compute $D \pluseq (AB)C$ instead.
Attempting to derive an algorithm for that problem directly does not work, as the block $\tilde{AB}$ in $L2$ cache would need to be recomputed multiple times for each iteration of the loop over $n$, and would also not see much reuse.
Fortunately, we can transform problems of this form to the equivalent problem $D^T \pluseq C^T(B^TA^T)$.
This transformation can be performed at effectively no cost by re-interpreting the memory the matrices are stored in (treating a row-major matrix as column-major and vice-versa).

\section{Experiments and Results}
To test our algorithm's performance, we implemented both it and the BLIS algorithm in the Multilevel Optimization for Matrix Multiply Sandbox (MOMMS), which wes developed for\cite{SmithDiss2017}.
This framework allowed us to declaratively generate the code for an algorithm by specifying the series of loops and packing operations required, and allowed us to interface to the BLIS microkernels.
Initially, we verified that the MOMMS implementation of the BLIS algorithm had similar performance to the reference implementation, which was written in C.

We modified the framework to add support for ``subcomputations'', which are virtual objects that represent a \gemm{} operation.
Subcomputations pass through partitionings to their respective input matrices, and then can be forced to fill a given output matrix.
This both allowed us to cleanly express the \gemmt{} algorithm, and allowed the typical $T = BC; D \pluseq AT$ algorithm for \gemmt{} to be expressed as the BLIS algorithm where one operand is the BLIS algorithm as a subcompuation, with that operand being forced immediately.

We tested the performance, in gigaflops per second, of both our algorithm and the typical approach.
The experiments were run on a public lab machine at the University of Texas at Austin.
The machine had an Intel Xeon E3-1270 v3 CPU, which ran at 3.5 GHz and implemented the Haswell microarchitecture.
The experimental system had 15 GB of RAM, 8 MB of $L3$ cache, 256 KB of $L2$ cache (which was an 8-way set-associative cache with 64 byte cache lines) and $32 KM$ of $L1$ cache (per core), which had the same characteristics as the $L2$.
The blocking constants that arose from this data can be found in Table \ref{tab:constants}

\begin{table}
  \centering
  \begin{tabular}{l|c c}
    &\gemmt{}&BLIS algorithm\\ \hline
    $m_C$&72&72\\
    $k_C$&252&256\\
    $l_C$&256&\\
    $n_C$&2040&4080\\
  \end{tabular}
  \caption{Blocking constants for Haswell}
  \label{tab:constants}
\end{table}

Our experiments were run both on square matrices ($m = n = k = l = N$) and on problems where a particular dimension was fixed at $9$ while the others varied together, to evaluate the performance of these algorithms on multiple input shapes.
We sampled the performance at multiples of sixteen from $N = 16$ to $4912$ to test the suitability of our approach on many input sizes.
All the inputs were stored column-major, while the temporaries and outputs were stored row-major, since the BLIS microkernel has noticably increased performance when writing to row-major matrices (and since the packed buffer is row-major).
This unusual combination of input formats ensured a fair comparison between the two algorithms.
In addition, we tested the $D^T = C^T(B^TA^T)$ case, in which all matrices were row major, for square inputs.

The results of these experiments can be found in Figure \ref{fig:bc_square} for square matrices, and Figure \ref{fig:bc_rectangles} for the rectangular inputs.
The $D \pluseq (AB)C$ experiment's results are in Figure \ref{fig:ab_square}.
Data for the square case shows that, once performance has stabilized around $N = 512$, the two algorithms are almost always within 5\% of each other in GFlops/s, showing comparable performance.

\begin{figure}
  \centering
  \includegraphics[height=0.40\textheight]{../results/earwig2/gemm3}
  \caption{Performance of our \gemmt{} implementation compared to a pairr of \gemm{} calls, square matrices}
  \label{fig:bc_square}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=0.40\textheight]{../results/earwig2/gemm3_rectangles}
  \caption{Performance of our \gemmt{} implementation compared to a pairr of \gemm{} calls, rectangular matrices}
  \label{fig:bc_rectangles}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=0.40\textheight]{../results/earwig2/gemm3_ab_bc_kernel}
  \caption{Performance of our \gemmt{} implementation compared to a pairr of \gemm{} calls, square transposed matrices}
  \label{fig:ab_square}
\end{figure}

The computed additional (excluding inputs and outputs) memory consumption of both algorithms for square matrices of various input sizes is shown in Figure \ref{fig:bc_square_mem}, demonstrating our approach's much lower memory usage.
\begin{figure}
  \centering
  \includegraphics[height=0.40\textheight]{../results/earwig2/gemm3_memory}
  \caption{Memory usage of our \gemmt{} implementation compared to a pairr of \gemm{} calls}
  \label{fig:bc_square_mem}
\end{figure}

\section{Discussion}
From the figures in the previous section, it is clear that \gemmt{} attains similar performance to a pair of \gemm{} calls with significantly lower memory usage.
However, it is important to dissect these performance results and determine why they have arisen.

For the square case, which was shown in Figure \ref{fig:bc_square}, we can observe that, for most of the 1000s and 2000s, the pair of \gemm{} calls has somewhat increased performance, but that the \gemmt{} algorithm is comparably or even more performant afterwards.
The higher performance for the \gemm{} calls arises from the lack of reuse of $\tilde{C}$ implied by the $252 \times l \cdot l \times n$ input to the inner algorithm for \gemmt{}.
Specifically, the first loop around the macrokernel in that algorithm always iterates at most four times, compared to the unbounded number of iterations in a call to \gemm{} to compute $BC$.
However, around when the temporary grows to 64 MB, the memory cost of writing it to memory becomes large enough that the performance of two \gemm{} calls begins to drop to and eventually falls below the performance of our \gemmt{} algorithm.
This shows that our approach, in addition to the memory savings, provides performance benefits for large matrix multiplications.

For small input sizes ($N < 512$), there is often a significant performance benefit attained by the \gemmt{} algorithm.
This performance improvement arises from the additional packing step that a pair of \gemm{} calls must perform, which has a non-negligible cost for these inputs, even if both the packed and unpacked data remain resident in the $L3$ cache.

Similar trends can be seen in Figure \ref{fig:ab_square}, which shows the results for $D \pluseq (AB)C$, implemented as $D^T \pluseq C^T(B^TA^T)$.
However, the pair of \gemm{} calls retains higher performance for a longer period of time on this workload.
This occurs because packing from row-major matrices to row-major packed buffers, which is something that occurs more frequently in this experiment, is much more amenable to the CPU's prefetcher than packing from column-major matrices.

The rectangular results from Figure \ref{fig:bc_rectangles} also require examination.
When $l$ is small, the \gemmt{} algorithm performs better on all inputs, as the memory-writing overhead in the BLIS algorithm remains, while the advantage of reuse in computing $BC$ is reduced (because the $k$ dimension is very narrow).
When $m$ is small, the costs of low $L3$ reuse are imposed at least once on both algorithms, decreasing the performance of both.

When $n$ is narrow, the problem is effectively converted to a series of several matrix-vector multiplications, which cannot achieve high performance with either algorithm.
For narrow $k$, the outer problem is an outer product, while the inner problem is a panel-matrix multiply (wide matrix times square matrix).
These shapes do not generally promote data reuse within the BLIS algorithm.
The eventually increased performance of \gemmt{} with $k = 9$ is due to the lowered number of memory operations that algorithm performs, since memory operations dominate this computation.

These results demonstrate the general suitability of our algorithm for \gemmt{} and its suitability as a high-performance primitive.

\section{Things that still might need doing}
\begin{itemize}
\item Portability (run on KNL)
\item Clean experiments (public lab machines are kinda a bad place)
\item Parallel case (the square-case performance behavior we have is still there, just much more exaggerated on multiple cores)
\item Make this somewhat longer to keep the writing flag people happy (should happen if we fix one of these other points)
\end{itemize}
\printbibliography{}
\end{document}
